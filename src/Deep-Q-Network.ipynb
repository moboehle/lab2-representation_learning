{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Inspired by and adapted to my needs from https://github.com/ageron\n",
    "#Based on Mnih 2013, playing atari with deep reinforcement learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import tensorflow as tf\n",
    "from importlib import reload\n",
    "sess = tf.InteractiveSession()\n",
    "import os\n",
    "import random\n",
    "from params import * \n",
    "import world \n",
    "from copy import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "from network import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#Precision at what the goal state will be accepted. For more info look at my_world.game_over()\n",
    "precision = 3\n",
    "world = reload(world)\n",
    "my_world = world.world(game_mode='constant_reward',goal_reward = 50,precision= precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Defining the actions\n",
    "\n",
    "def increase_height():\n",
    "    #change in pixel\n",
    "    my_world.change_height(1)\n",
    "def decrease_height():\n",
    "    #change in pixel\n",
    "    my_world.change_height(-1)\n",
    "def increase_width():\n",
    "    #change in pixel\n",
    "    my_world.change_width(1)\n",
    "def decrease_width():\n",
    "    #change in pixel\n",
    "    my_world.change_width(-1)\n",
    "    \n",
    "def increase_height2():\n",
    "    #change in pixel\n",
    "    my_world.change_height(1,ellipse=1)\n",
    "def decrease_height2():\n",
    "    #change in pixel\n",
    "    my_world.change_height(-1,ellipse=1)\n",
    "def increase_width2():\n",
    "    #change in pixel\n",
    "    my_world.change_width(1,ellipse=1)\n",
    "def decrease_width2():\n",
    "    #change in pixel\n",
    "    my_world.change_width(-1,ellipse=1)\n",
    "\n",
    "    \n",
    "#List of actions to use for the network.\n",
    "actions = [increase_height,decrease_height,increase_width,decrease_width,\\\n",
    "          increase_height2,decrease_height2,increase_width2,decrease_width2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the model architecture look like?\n",
    "\n",
    "- We have the input with dimensions [FRAME_DIM,FRAME_DIM,1] (in the DQN of Mnih, the 4 last images are taken. However, our task is much simpler and does not require to learn any correlations between the images, so 1 is sufficient as the last dimension (taking just 1 image.)\n",
    "- Moreover, in our case, it is sufficient to take $\\Phi$ to be the identity function, since we do not need preprocessing for image cropping etc.\n",
    "\n",
    "\n",
    "- First hidden layer: 16 filters of size 8x8 with stride 4. acitvation : relu\n",
    "- Second hidden layer: 32 filters of size 4x4 with stride 2, again relu (EXCLUDED FOR NOW)\n",
    "- Final hidden 128 relus, fully connected.\n",
    "- output layer: fully connected, one output for each action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dqn_args   = {\"n_out_h1\" : 16, \"kernel_size_h1\":(8,8), \"strides_h1\" : 4, \"padding\" : \"SAME\", \\\n",
    "              \"actvt_fct\" : tf.nn.relu, \"n_out_h2\" : 32, \"kernel_size_h2\":(4,4), \"strides_h2\" : 2, \\\n",
    "              \"n_out_h3\" : 128,\"initializer\" : tf.contrib.layers.variance_scaling_initializer() }\n",
    "\n",
    "\n",
    "num_actions = len(actions)\n",
    "num_targets = 3\n",
    "targets = [{\"vaxis\" : vaxis, \"haxis\" : haxis,\"vaxis2\":vaxis2,\"haxis2\":haxis2} for vaxis,haxis,vaxis2,haxis2 in \\\n",
    "           zip(np.linspace(5,25,num_targets),np.linspace(5,20,num_targets),np.linspace(5,20,num_targets),np.linspace(5,20,num_targets))]\n",
    "\n",
    "\n",
    "learning_rate  = 1*10**(-3)\n",
    "\n",
    "#Batch of memories drawn from memory buffer\n",
    "batch_size     = 32 \n",
    "\n",
    "# start training after memory is somewhat filled.\n",
    "training_start = 4*batch_size  \n",
    "# gamma discount factor for future rewards. \n",
    "discount_rate  = .9\n",
    "\n",
    "\n",
    "over = True\n",
    "\n",
    "# We will keep track of the max Q-Value over time and compute the mean per game\n",
    "prediction_loss = np.infty\n",
    "game_length = 0\n",
    "finished   = -1\n",
    "\n",
    "\n",
    "\n",
    "num_total_updates = 100000\n",
    "verbose = True\n",
    "\n",
    "#Time interval after which the target network gets updated by the online learning network\n",
    "learn_period = 20\n",
    "\n",
    "\n",
    "game_length_over_t = []\n",
    "\n",
    "time = []\n",
    "t = 0 \n",
    "\n",
    "#Epsilon greedy policy: initially fully exploratory then decaying epsilon\n",
    "eps_min = 0.1\n",
    "eps_max = 1.0 \n",
    "exploratory_steps = num_total_updates//6\n",
    "eps_decay_steps = num_total_updates-exploratory_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#x is our current state, i.e. just the current image.\n",
    "#Same for learning network as for predicting network, therefore defined outside the scope so that it can be shared.\n",
    "\n",
    "learner_out  , learner_vars, _   = network(\"learner\"  ,**dqn_args,num_actions=num_actions,num_targets=num_targets)\n",
    "predictor_out, predictor_vars, _ = network(\"predictor\",**dqn_args,num_actions=num_actions,num_targets=num_targets)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#clever way in tensorflow of updating the predictor network!\n",
    "#Since all operations are just stored as nodes in the graph, the reassignment operations can be put into \n",
    "#a single group of operations. Whenever this group is then called during a session, the assignment node\n",
    "# gets activated -> Transfer of current learning parameter values into the predicting network.\n",
    "update_operations = [predictor_var.assign(learner_vars[var_name])\n",
    "                    for var_name, predictor_var in predictor_vars.items()]\n",
    "\n",
    "update_predictor = tf.group(*update_operations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training step of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Define all the variables that will be used for the training, e.g. actions taken, target, q_value and so on.\n",
    "\n",
    "with tf.variable_scope(\"training\",reuse=tf.AUTO_REUSE):\n",
    "    \n",
    "    #Placeholder for batch of actions during training.\n",
    "    act_idx = tf.placeholder(tf.int32, shape=[None])\n",
    "    #Placeholder for estimated discounted future reward achieved in a step.\n",
    "    y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "    \n",
    "    # The q_value of the online learning network for a certain step.\n",
    "    q_value_per_target = [tf.reduce_sum(output * tf.one_hot(act_idx, num_actions), axis=1, keep_dims=True)\\\n",
    "                         for output in learner_out]\n",
    "    \n",
    "    ### As in the paper by Mnih, loss is squared difference between target and estimate. \n",
    "    loss_per_target = [tf.clip_by_value(tf.squared_difference(y,q_value,\"loss\"),0,100) for q_value in q_value_per_target]\n",
    "    \n",
    "    #Keeping track of how many batches the network has seen for training.\n",
    "    global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "    # Optimizer used by Mnih\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate)\n",
    "    training_step_per_target = [optimizer.minimize(loss, global_step=global_step) for loss in loss_per_target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Memory for replay\n",
    "max_memory = 500\n",
    "#Using deque so that max_length is automatically maintained when pushing new data into the replay buffer.\n",
    "memory = [deque([],maxlen=max_memory) for _ in range(num_targets)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_memories(mem,batch_size):\n",
    "    \n",
    "    '''\n",
    "    Sample memories from the replay buffer. Returns a batch of batch_size that includes the \n",
    "    remembered state-action-reward-next_state-game_over tuple.\n",
    "    '''\n",
    "    \n",
    "    memo_batch  = np.array(random.sample(mem,batch_size))\n",
    "    \n",
    "    mem_states       = np.stack(np.array(memo_batch[:,0]))\n",
    "    mem_actions      = np.stack(np.array(memo_batch[:,1]))\n",
    "    mem_rewards      = np.stack(np.array(memo_batch[:,2]))\n",
    "    mem_nxt_state    = np.stack(np.array(memo_batch[:,3]))\n",
    "    mem_final_state  = np.stack(np.array(memo_batch[:,4]))\n",
    "\n",
    "    \n",
    "    return (mem_states, mem_actions, mem_rewards.reshape(-1, 1), mem_nxt_state, mem_final_state.reshape(-1, 1))\n",
    "\n",
    "\n",
    "def epsilon_greedy(q_values, num_updates):\n",
    "    '''\n",
    "    Custom epsilon greedy policy. Learn completely random in the first \"exploratory_steps\" number of steps to\n",
    "    explore the state space more or less equally. Then decay to minimal epsilon to become more and more greedy.\n",
    "    '''\n",
    "    \n",
    "    #First completely random, then decaying linearly from eps_max to eps_min\n",
    "    epsilon = 1 if num_updates< exploratory_steps else  \\\n",
    "                max(eps_min, eps_max - (eps_max-eps_min) * (num_updates-exploratory_steps)/eps_decay_steps)\n",
    "        \n",
    "    if np.random.rand() < epsilon:\n",
    "        # explore\n",
    "        return np.random.randint(num_actions) \n",
    "    else:\n",
    "        # exploit\n",
    "        return np.argmax(q_values) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!rm -r one-ellipse/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_path = \"two-ellipses/\"\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_at = 0\n",
    "current_target = 0\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    if os.path.isfile(save_path+\".index\"):\n",
    "        saver.restore(sess, save_path)\n",
    "        start_at = global_step.eval()\n",
    "        print(\"Retrieving saved parameters\")\n",
    "    else:\n",
    "        init.run()\n",
    "        \n",
    "    #start with online being the same as predictor network\n",
    "    update_predictor.run()\n",
    "    #start_at = 0\n",
    "    print(\"Starting learning process - Filling memory.\",end=\"\")\n",
    "    \n",
    "    while True:\n",
    "        t +=1\n",
    "        num_updates = global_step.eval()\n",
    "        if num_updates-start_at >= num_total_updates:\n",
    "            saver.save(sess, save_path)\n",
    "            break\n",
    "            \n",
    "        #Reset world if goal is reached  \n",
    "        if over:            \n",
    "            finished +=1\n",
    "            my_world.restart()\n",
    "            state = my_world.get_frame()\n",
    "\n",
    "\n",
    "        #Perform actions according to online network and our custom epsilon greedy policy.\n",
    "        q_values = learner_out[current_target].eval(feed_dict={x: [state]})\n",
    "        action_idx = epsilon_greedy(q_values, num_updates-start_at)\n",
    "        #Perform action\n",
    "        actions[action_idx]()\n",
    "        #observe world...\n",
    "        next_state, reward, over = (my_world.get_frame(),\\\n",
    "                                    my_world.get_reward(),\\\n",
    "                                    my_world.game_over())\n",
    "\n",
    "        #Place observations in memory.\n",
    "        memory[current_target].append((state, action_idx, reward, next_state, not over))\n",
    "\n",
    "        \n",
    "        state = world.get_frame()\n",
    "\n",
    "        \n",
    "        \n",
    "        #Keep track of game_length over time\n",
    "        game_length += 1\n",
    "\n",
    "        if over:\n",
    "            game_length_over_t.append(game_length)\n",
    "            time.append(t)\n",
    "            game_length = 0\n",
    "\n",
    "        if np.any([len(mem) < training_start for mem in memory ]):\n",
    "            \n",
    "\n",
    "            #skip first \"training_start\" steps until memory is filled enough\n",
    "            current_target+=1\n",
    "            current_target%=num_targets\n",
    "            my_world.set_target(**targets[current_target])\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        #Print progress!\n",
    "        elif verbose and num_updates % 200 == 0:\n",
    "            print(\"\\r Update {}/{} loss : {:.2f} \"\n",
    "                  \" finished games:{} width {:.1f} height {:.1f} reward {:.2f}\".format(\n",
    "             num_updates-start_at, num_total_updates,np.sum(prediction_loss)/batch_size,\n",
    "             finished,my_world.haxis,my_world.vaxis,reward), end=\"\")\n",
    "            \n",
    "            \n",
    "        # Sample memories and use the target DQN to produce the target Q-Value\n",
    "        mem_x, mem_action, mem_rewards, mem_next_state, continues = sample_memories(memory[current_target],batch_size)\n",
    "        \n",
    "        #For target estimate use the predictor #bootstrappingFutureRewards..\n",
    "        next_q_values = predictor_out[current_target].eval(feed_dict={x: mem_next_state})\n",
    "        #Learn according to greedy policy.\n",
    "        \n",
    "        max_next_q_values = np.max(next_q_values, axis=1, keepdims=True)\n",
    "        \n",
    "        #Target to achieve. \n",
    "        y_val = mem_rewards + continues * discount_rate * max_next_q_values\n",
    "\n",
    "        #Perform training steps and calculate the loss\n",
    "        _, prediction_loss = sess.run([training_step_per_target[current_target],\\\n",
    "                                       loss_per_target[current_target]], \n",
    "                                      feed_dict={x: mem_x, act_idx: mem_action, y: y_val})\n",
    "        # Update the predicting network every once in a while \n",
    "        if num_updates % learn_period == 0:\n",
    "            update_predictor.run()\n",
    "\n",
    "        # save tuned variables.\n",
    "        if num_updates % 1000 == 0:\n",
    "            current_target+=1\n",
    "            current_target%=num_targets\n",
    "            my_world.set_target(**targets[current_target])\n",
    "            \n",
    "            saver.save(sess, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Plotting the reward function over parameter space\n",
    "\n",
    "my_world.set_target(**targets[0])\n",
    "range_x  = int(FRAME_DIM/2-MARGIN/2)\n",
    "range_y  = int(FRAME_DIM/2-MARGIN/2)\n",
    "parameter_space = np.array([(haxis,vaxis) for haxis in np.arange(1,range_x+1)\\\n",
    "                                          for vaxis in np.arange(1,range_y+1)])\n",
    "\n",
    "reward_per_target     = [np.zeros((range_x+1,range_y+1)) for _ in range(len(targets))]\n",
    "\n",
    "for ellipse in parameter_space:\n",
    "    my_world.haxis = ellipse[0]\n",
    "    my_world.vaxis = ellipse[1]\n",
    "\n",
    "    my_world.set_frame(which=\"first\")\n",
    "\n",
    "\n",
    "    for idx in range(len(targets)):\n",
    "       \n",
    "        reward_per_target[idx][ellipse[0]][ellipse[1]] = my_world.get_reward()\n",
    "       \n",
    "plt.imshow(reward_per_target[0][1:,1:],origin=\"lower\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(time, game_length_over_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO refactor this, let network play greedy.\n",
    "\n",
    "game_length = 0\n",
    "play_game_length_over_t = []\n",
    "play_time = []\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    if os.path.isfile(\"./.index\"):\n",
    "        saver.restore(sess, './')\n",
    "        \n",
    "        \n",
    "        for t in range(2000):\n",
    "            #Reset world if goal is reached  \n",
    "            if over:            \n",
    "                finished +=1\n",
    "                my_world.restart()\n",
    "                state = my_world.get_frame()\n",
    "                \n",
    "\n",
    "            #Perform actions according to online network and our custom epsilon greedy policy.\n",
    "            q_values = predictor_out.eval(feed_dict={x: [state]})\n",
    "            action_idx = np.argmax(q_values)\n",
    "            #Perform action\n",
    "            actions[action_idx]()\n",
    "            #observe world...\n",
    "            state, reward, over = (my_world.get_frame(),\\\n",
    "                                        my_world.get_reward(),\\\n",
    "                                        my_world.game_over(precision=precision))\n",
    "\n",
    "           \n",
    "            #Keep track of game_length over time\n",
    "            game_length += 1\n",
    "\n",
    "            if over:\n",
    "                play_game_length_over_t.append(game_length)\n",
    "                play_time.append(t)\n",
    "                game_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(play_time,play_game_length_over_t)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
